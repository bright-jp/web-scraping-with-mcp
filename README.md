# Anthropicã®MCPã«ã‚ˆã‚‹Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

[![Bright Data Promo](https://github.com/bright-jp/LinkedIn-Scraper/raw/main/Proxies%20and%20scrapers%20GitHub%20bonus%20banner.png)](https://brightdata.jp/)

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã®ãŸã‚ã«MCPã‚µãƒ¼ãƒãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã€é–‹ç™ºãƒ„ãƒ¼ãƒ«ã¨æ¥ç¶šã—ã€Bright Dataã‚’æ´»ç”¨ã—ã¦AIäº’æ›ã®Webæƒ…å ±ã‚’å³åº§ã«å–å¾—ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

- [åˆ¶ç´„ã®ç†è§£ï¼šãªãœLLMã¯ç¾å®Ÿä¸–ç•Œã¨ã®ç›¸äº’ä½œç”¨ã«æ”¯æ´ãŒå¿…è¦ãªã®ã‹](#understanding-the-limitation-why-llms-need-help-with-real-world-interaction)
- [MCPã®é‡è¦æ€§](#the-importance-of-mcp)
- [Model Context Protocolã®ç†è§£](#understanding-model-context-protocol)
- [MCPã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è§£èª¬](#mcp-architecture-explained)
- [ç‹¬è‡ªã®MCPã‚µãƒ¼ãƒãƒ¼ã‚’é–‹ç™ºã™ã‚‹](#developing-your-own-mcp-server)
- [MCPã‚µãƒ¼ãƒãƒ¼ã‚’æ¥ç¶šã™ã‚‹](#connecting-your-mcp-server)
- [ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªWebãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã®ãŸã‚ã«Bright Dataã®MCPã‚’ä½¿ã†](#using-bright-datas-mcp-for-professional-web-data-extraction)
- [å‚è€ƒè³‡æ–™](#further-reading)

## Understanding the Limitation: Why LLMs Need Help with Real-World Interaction

Large Language Modelsï¼ˆLLMï¼‰ã¯ã€åºƒç¯„ãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ãƒ»ç”Ÿæˆã™ã‚‹ã“ã¨ã«å„ªã‚Œã¦ã„ã¾ã™ã€‚ã—ã‹ã—ã€é‡å¤§ãªåˆ¶ç´„ãŒã‚ã‚Šã¾ã™ã€‚ãã‚Œã¯ã€å¤–éƒ¨ä¸–ç•Œã¨è‡ªç„¶ã«ç›¸äº’ä½œç”¨ã§ããªã„ã“ã¨ã§ã™ã€‚ã¤ã¾ã‚Šã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã€ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œã€Webã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã¨ã„ã£ãŸæ©Ÿèƒ½ãŒæ¨™æº–ã§ã¯å‚™ã‚ã£ã¦ã„ã¾ã›ã‚“ã€‚

åŸºæœ¬çš„ãªä¾‹ã¨ã—ã¦ã€Claudeã«ç¨¼åƒä¸­ã®Amazonå•†å“ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°ã‚’æŠ½å‡ºã™ã‚‹ã‚ˆã†ä¾é ¼ã—ã¦ã‚‚ã€è¿½åŠ ãƒ„ãƒ¼ãƒ«ãªã—ã§ã¯ä¸å¯èƒ½ã§ã™ã€‚ãªãœã§ã—ã‚‡ã†ã‹ã€‚ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‚’é–²è¦§ã—ãŸã‚Šå¤–éƒ¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒˆãƒªã‚¬ãƒ¼ã—ãŸã‚Šã™ã‚‹å›ºæœ‰ã®èƒ½åŠ›ãŒãªã„ãŸã‚ã§ã™ã€‚

![claude-without-mcp](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/claude-without-mcp.png)

è£œåŠ©ãƒ„ãƒ¼ãƒ«ãŒãªã‘ã‚Œã°ã€LLMã¯ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ã‚„å¤–éƒ¨ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆã«ä¾å­˜ã™ã‚‹å®Ÿç”¨çš„ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã€‚

ãã“ã§ä¾¡å€¤ã‚’ç™ºæ®ã™ã‚‹ã®ãŒã€[Anthropicã®Model Context Protocolï¼ˆMCPï¼‰](https://www.anthropic.com/news/model-context-protocol)ã§ã™ã€‚MCPã¯ã€ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ„ãƒ¼ãƒ«ã€APIã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆãªã©ã®å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã¨ã€LLMãŒå®‰å…¨ã‹ã¤æ¨™æº–åŒ–ã•ã‚ŒãŸæ–¹æ³•ã§é€šä¿¡ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

å®Ÿéš›ã®é•ã„ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚ã‚«ã‚¹ã‚¿ãƒ MCPã‚µãƒ¼ãƒãƒ¼ã‚’çµ±åˆã—ãŸå¾Œã€Claudeã‹ã‚‰ç›´æ¥ã€æ§‹é€ åŒ–ã•ã‚ŒãŸAmazonå•†å“æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ã“ã¨ã«æˆåŠŸã—ã¾ã—ãŸã€‚

![claude-amazon-product-data-extraction-results](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/claude-amazon-product-data-extraction-results.png)

## The Importance of MCP

- **æ¨™æº–åŒ–:** MCPã¯ã€LLMãƒ™ãƒ¼ã‚¹ã®ã‚·ã‚¹ãƒ†ãƒ ãŒå¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã«æ¥ç¶šã™ã‚‹ãŸã‚ã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚Webçµ±åˆã‚’APIãŒæ¨™æº–åŒ–ã—ãŸã®ã¨åŒæ§˜ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã‚«ã‚¹ã‚¿ãƒ çµ±åˆã®å¿…è¦æ€§ãŒå¤§å¹…ã«æ¸›ã‚Šã€é–‹ç™ºãŒåŠ é€Ÿã—ã¾ã™ã€‚
- **æŸ”è»Ÿæ€§ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£:** é–‹ç™ºè€…ã¯ã€LLMã‚„ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’ç½®ãæ›ãˆã¦ã‚‚ã€ãƒ„ãƒ¼ãƒ«çµ±åˆã‚’æ›¸ãç›´ã™å¿…è¦ãŒã‚ã‚Šã¾ã›ã‚“ã€‚MCPã¯`stdio`ãªã©è¤‡æ•°ã®é€šä¿¡æ–¹å¼ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€ã•ã¾ã–ã¾ãªæ§‹æˆã«é©å¿œã§ãã¾ã™ã€‚
- **LLMæ©Ÿèƒ½ã®å¼·åŒ–:** MCPã§LLMã‚’æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚„å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã«æ¥ç¶šã™ã‚‹ã“ã¨ã§ã€é™çš„ãªå›ç­”ã‚’è¶…ãˆã‚‰ã‚Œã¾ã™ã€‚æ–‡è„ˆã«åŸºã¥ã„ã¦æœ€æ–°ã§é–¢é€£æ€§ã®é«˜ã„æƒ…å ±ã‚’æä¾›ã—ã€ç¾å®Ÿä¸–ç•Œã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒˆãƒªã‚¬ãƒ¼ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

> **ãŸã¨ãˆè©±**:
> 
> MCPã¯LLMã®ãŸã‚ã®USBã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã ã¨è€ƒãˆã¦ãã ã•ã„ã€‚USBãŒï¼ˆã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã€ãƒ—ãƒªãƒ³ã‚¿ãƒ¼ã€å¤–ä»˜ã‘ãƒ‰ãƒ©ã‚¤ãƒ–ãªã©ã®ï¼‰ç•°ãªã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’ç‰¹åˆ¥ãªãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãªã—ã§äº’æ›ãƒã‚·ãƒ³ã«æ¥ç¶šã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã®ã¨åŒæ§˜ã«ã€MCPã¯æ¨™æº–åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ç”¨ã„ã¦LLMã‚’å¹…åºƒã„ãƒ„ãƒ¼ãƒ«ã«æ¥ç¶šã§ãã¾ã™ã€‚æ¯å›ã‚«ã‚¹ã‚¿ãƒ çµ±åˆã‚’è¡Œã†å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

## Understanding Model Context Protocol

Model Context Protocolï¼ˆMCPï¼‰ã¯ã€AnthropicãŒé–‹ç™ºã—ãŸã‚ªãƒ¼ãƒ—ãƒ³ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã§ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ãŒå¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã€APIã€ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¨ä¸€è²«æ€§ãŒã‚ã‚Šå®‰å…¨ãªæ–¹æ³•ã§ç›¸äº’ä½œç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚ãƒ¦ãƒ‹ãƒãƒ¼ã‚µãƒ«ã‚³ãƒã‚¯ã‚¿ãƒ¼ã¨ã—ã¦æ©Ÿèƒ½ã—ã€Webã‚µã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¯ã‚¨ãƒªã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œã¨ã„ã£ãŸç¾å®Ÿä¸–ç•Œã®ã‚¿ã‚¹ã‚¯ã‚’LLMãŒè¡Œãˆã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

AnthropicãŒå°å…¥ã—ãŸã‚‚ã®ã§ã™ãŒã€MCPã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‹ã¤æ‹¡å¼µå¯èƒ½ã§ã‚ã‚Šã€èª°ã§ã‚‚æ¨™æº–ã‚’å®Ÿè£…ãƒ»è²¢çŒ®ã§ãã¾ã™ã€‚[Retrieval-Augmented Generationï¼ˆRAGï¼‰](https://brightdata.jp/blog/web-data/rag-explained)ã‚’æ‰±ã£ãŸã“ã¨ãŒã‚ã‚‹æ–¹ãªã‚‰ã€ã“ã®æ¦‚å¿µã‚’ç†è§£ã—ã‚„ã™ã„ã¯ãšã§ã™ã€‚MCPã¯ãã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç™ºå±•ã•ã›ã€è»½é‡ãªJSON-RPCã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã«ã‚ˆã£ã¦ç›¸äº’ä½œç”¨ã‚’æ¨™æº–åŒ–ã—ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒ©ã‚¤ãƒ–ãƒ‡ãƒ¼ã‚¿ã¸ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

## MCP Architecture Explained

MCPã®åŸºç›¤ã¯ã€AIãƒ¢ãƒ‡ãƒ«ã¨å¤–éƒ¨æ©Ÿèƒ½ã®é–“ã®é€šä¿¡ã‚’æ¨™æº–åŒ–ã™ã‚‹ã“ã¨ã«ã‚ã‚Šã¾ã™ã€‚

**ä¸­æ ¸ã®ã‚¢ã‚¤ãƒ‡ã‚¢:** æ¨™æº–åŒ–ã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆé€šå¸¸ã¯`stdio`ã®ã‚ˆã†ãªãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ãƒˆä¸Šã®JSON-RPC 2.0ï¼‰ã«ã‚ˆã‚Šã€LLMï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆçµŒç”±ï¼‰ã¯å¤–éƒ¨ã‚µãƒ¼ãƒãƒ¼ãŒå…¬é–‹ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’ç™ºè¦‹ã—ã€å‘¼ã³å‡ºã›ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

MCPã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ/ã‚µãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§å‹•ä½œã—ã€ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯3ã¤ã§ã™ã€‚

1. **MCP Host**: LLMã¨å¤–éƒ¨ãƒ„ãƒ¼ãƒ«é–“ã®ç›¸äº’ä½œç”¨ã‚’é–‹å§‹ãƒ»ç®¡ç†ã™ã‚‹ç’°å¢ƒã¾ãŸã¯ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚ä¾‹ã¨ã—ã¦ã€_Claude Desktop_ã®ã‚ˆã†ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚„ã€_Cursor_ã®ã‚ˆã†ãªIDEãŒã‚ã‚Šã¾ã™ã€‚
2. **MCP Client**: hostå†…ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã€MCP Serverã¸ã®æ¥ç¶šã‚’ç¢ºç«‹ãƒ»ç¶­æŒã—ã€é€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’å‡¦ç†ã—ã¦ãƒ‡ãƒ¼ã‚¿äº¤æ›ã‚’ç®¡ç†ã—ã¾ã™ã€‚
3. **MCP Server:** ï¼ˆé–‹ç™ºè€…ã§ã‚ã‚‹ç§ãŸã¡ãŒä½œæˆã™ã‚‹ï¼‰MCPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’å®Ÿè£…ã—ã€ç‰¹å®šã®æ©Ÿèƒ½ã‚»ãƒƒãƒˆã‚’å…¬é–‹ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ã™ã€‚MCP serverã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€Webã‚µãƒ¼ãƒ“ã‚¹ã€ã¾ãŸã¯ã“ã®ã‚±ãƒ¼ã‚¹ã§ã¯Webã‚µã‚¤ãƒˆï¼ˆAmazonï¼‰ã¨é€£æºã§ãã¾ã™ã€‚serverã¯æ©Ÿèƒ½ã‚’æ¨™æº–åŒ–ã•ã‚ŒãŸå½¢ã§å…¬é–‹ã—ã¾ã™:
   - **Tools:** å‘¼ã³å‡ºã—å¯èƒ½ãªé–¢æ•°ï¼ˆä¾‹: _scrape\_amazon\_product_, _get\_weather\_data_ï¼‰
   - **Resources:** é™çš„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®èª­ã¿å–ã‚Šå°‚ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆä¾‹: ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã€JSONãƒ¬ã‚³ãƒ¼ãƒ‰ã®è¿”å´ï¼‰
   - **Prompts:** LLMãŒtoolsã‚„resourcesã¨ã‚„ã‚Šå–ã‚Šã™ã‚‹éš›ã®ã‚¬ã‚¤ãƒ‰ã¨ãªã‚‹äº‹å‰å®šç¾©ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

MCPã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³ã¯ã“ã¡ã‚‰ã§ã™ã€‚

![mcp-architecture-diagram-host-client-server-connections](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/mcp-architecture-diagram-host-client-server-connections.png)

_Image Source: [Model Context Protocol](https://modelcontextprotocol.io/introduction)_

ã“ã®æ§‹æˆã§ã¯ã€**host**ï¼ˆClaude Desktopã¾ãŸã¯Cursor IDEï¼‰ãŒ**MCP client**ã‚’èµ·å‹•ã—ã€ãã‚ŒãŒå¤–éƒ¨ã®**MCP server**ã«æ¥ç¶šã—ã¾ã™ã€‚serverã¯toolsã€resourcesã€promptsã‚’å…¬é–‹ã—ã€AIãŒå¿…è¦ã«å¿œã˜ã¦ãã‚Œã‚‰ã¨ç›¸äº’ä½œç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

è¦ã™ã‚‹ã«ã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¯æ¬¡ã®ã‚ˆã†ã«å‹•ä½œã—ã¾ã™ã€‚

- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ _ã€Œã“ã®Amazonãƒªãƒ³ã‚¯ã®å•†å“æƒ…å ±ã‚’å–å¾—ã—ã¦ã€‚ã€_ ã®ã‚ˆã†ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã—ã¾ã™
- MCP clientãŒã€ãã®ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†ã§ãã‚‹ç™»éŒ²æ¸ˆã¿toolã‚’ç¢ºèªã—ã¾ã™
- clientãŒæ§‹é€ åŒ–ã•ã‚ŒãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’MCP serverã¸é€ä¿¡ã—ã¾ã™
- MCP serverãŒé©åˆ‡ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆä¾‹: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ï¼‰
- serverãŒæ§‹é€ åŒ–ã•ã‚ŒãŸçµæœã‚’MCP clientã¸è¿”ã—ã¾ã™
- clientãŒçµæœã‚’LLMã¸æ¸¡ã—ã€LLMãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æç¤ºã—ã¾ã™

## Developing Your Own MCP Server

Amazonã®å•†å“ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹Pythonã®MCP serverã‚’æ§‹ç¯‰ã—ã¾ã—ã‚‡ã†ã€‚

![amazon-product-page-example](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/amazon-product-page-example.png)

ã“ã®serverã¯2ã¤ã®toolã‚’æä¾›ã—ã¾ã™ã€‚1ã¤ã¯HTMLã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã€ã‚‚ã†1ã¤ã¯æ•´ç†ã•ã‚ŒãŸæƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚Cursorã¾ãŸã¯Claude Desktopã®LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä»‹ã—ã¦serverã¨ã‚„ã‚Šå–ã‚Šã—ã¾ã™ã€‚

### Step 1: Preparing Your Environment

ã¾ãšã€[Python 3](https://www.python.org/downloads/)ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚æ¬¡ã«ã€ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆã—ã¦æœ‰åŠ¹åŒ–ã—ã¾ã™ã€‚

```sh
python -m venv mcp-amazon-scraper
# On macOS/Linux:
source mcp-amazon-scraper/bin/activate
# On Windows:
.\mcp-amazon-scraper\Scripts\activate
```

å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆ[MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)ã€[Playwright](https://playwright.dev/python/)ã€[LXML](https://lxml.de/)ï¼‰ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

```sh
pip install mcp playwright lxml
# Install browser binaries for Playwright
python -m playwright install
```

ã“ã‚Œã«ã‚ˆã‚Šæ¬¡ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¾ã™ã€‚

- **mcp**: Model Context Protocolã®serverãŠã‚ˆã³clientå‘ã‘Python SDKã§ã€JSON-RPCé€šä¿¡ã®è©³ç´°ã‚’ã™ã¹ã¦å‡¦ç†ã—ã¾ã™
- **playwright**: ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€JavaScriptãŒå¤šã„Webã‚µã‚¤ãƒˆã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™
- **lxml**: é«˜é€ŸãªXML/HTMLãƒ‘ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€XPathã‚¯ã‚¨ãƒªã‚’ä½¿ã£ã¦Webãƒšãƒ¼ã‚¸ã‹ã‚‰ç‰¹å®šãƒ‡ãƒ¼ã‚¿è¦ç´ ã‚’ç°¡å˜ã«æŠ½å‡ºã§ãã¾ã™

è¦ã™ã‚‹ã«ã€MCP Python SDKï¼ˆ`mcp`ï¼‰ãŒãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®è©³ç´°ã‚’ã™ã¹ã¦å‡¦ç†ã™ã‚‹ãŸã‚ã€Claudeã‚„CursorãŒè‡ªç„¶è¨€èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰å‘¼ã³å‡ºã›ã‚‹toolã‚’å…¬é–‹ã§ãã¾ã™ã€‚Playwrightã¯Webãƒšãƒ¼ã‚¸ï¼ˆJavaScriptã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å«ã‚€ï¼‰ã‚’å®Œå…¨ã«ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã§ãã€lxmlã¯å¼·åŠ›ãªHTMLãƒ‘ãƒ¼ã‚¹æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

### Step 2: Starting the MCP Server

`amazon_scraper_mcp.py`ã¨ã„ã†åå‰ã®Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚ã¾ãšã€å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã€`FastMCP` serverã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

```python
import os
import asyncio
from lxml import html as lxml_html
from mcp.server.fastmcp import FastMCP
from playwright.async_api import async_playwright

# Define a temporary file path for the HTML content
HTML_FILE = os.path.join(os.getenv("TMPDIR", "/tmp"), "amazon_product_page.html")

# Initialize the MCP server with a descriptive name
mcp = FastMCP("Amazon Product Scraper")

print("MCP Server Initialized: Amazon Product Scraper")
```

ã“ã‚Œã§MCP serverã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒä½œæˆã•ã‚Œã¾ã™ã€‚æ¬¡ã«toolã‚’è¿½åŠ ã—ã¾ã™ã€‚

### Step 3: Implementing the `fetch_page` Tool

ã“ã®toolã¯URLã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€Playwrightã§ãƒšãƒ¼ã‚¸ã¸ç§»å‹•ã—ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ­ãƒ¼ãƒ‰ã‚’å¾…ã¡ã€HTMLã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¸ä¿å­˜ã—ã¾ã™ã€‚

```python
@mcp.tool()
async def fetch_page(url: str) -> str:
    """
    Fetches the HTML content of the given Amazon product URL using Playwright
    and saves it to a temporary file. Returns a status message.
    """
    print(f"Executing fetch_page for URL: {url}")
    try:
        async with async_playwright() as p:
            # Launch headless Chromium browser
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            # Navigate to the URL with a generous timeout
            await page.goto(url, timeout=90000, wait_until="domcontentloaded")
            # Wait for a key element (e.g., body) to ensure basic loading
            await page.wait_for_selector("body", timeout=30000)
            # Add a small delay for any dynamic content rendering via JavaScript
            await asyncio.sleep(5)

            html_content = await page.content()
            with open(HTML_FILE, "w", encoding="utf-8") as f:
                f.write(html_content)

            await browser.close()
            print(f"Successfully fetched and saved HTML to {HTML_FILE}")
            return f"HTML content for {url} downloaded and saved successfully to {HTML_FILE}."
    except Exception as e:
        error_message = f"Error fetching page {url}: {str(e)}"
        print(error_message)
        return error_message
```

ã“ã®éåŒæœŸé–¢æ•°ã¯ã€Amazonãƒšãƒ¼ã‚¸ã§èµ·ã“ã‚Šå¾—ã‚‹JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’Playwrightã§å‡¦ç†ã—ã¾ã™ã€‚`@mcp.tool()`ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã«ã‚ˆã‚Šã€ã“ã®é–¢æ•°ã¯serverå†…ã§å‘¼ã³å‡ºã—å¯èƒ½ãªtoolã¨ã—ã¦ç™»éŒ²ã•ã‚Œã¾ã™ã€‚

### Step 4: Implementing the `extract_info` Tool

ã“ã®toolã¯ã€`fetch_page`ãŒä¿å­˜ã—ãŸHTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€LXMLã¨XPathã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§ãƒ‘ãƒ¼ã‚¹ã—ã€æŠ½å‡ºã—ãŸå•†å“è©³ç´°ã‚’å«ã‚€è¾æ›¸ã‚’è¿”ã—ã¾ã™ã€‚

```python
def _extract_xpath(tree, xpath, default="N/A"):
    """Helper function to extract text using XPath, returning default if not found."""
    try:
        # Use text_content() to get text from node and children, strip whitespace
        result = tree.xpath(xpath)
        if result:
            return result[0].text_content().strip()
        return default
    except Exception:
        return default

def _extract_price(price_str):
    """Helper function to parse price string into a float."""
    if price_str == "N/A":
        return None
    try:
        # Remove currency symbols and commas, handle potential whitespace
        cleaned_price = "".join(filter(str.isdigit or str.__eq__("."), price_str))
        return float(cleaned_price)
    except (ValueError, TypeError):
        return None

@mcp.tool()
def extract_info() -> dict:
    """
    Parses the saved HTML file (downloaded by fetch_page) to extract
    Amazon product details like title, price, rating, features, etc.
    Returns a dictionary of the extracted data.
    """
    print(f"Executing extract_info from file: {HTML_FILE}")
    if not os.path.exists(HTML_FILE):
        return {
            "error": f"HTML file not found at {HTML_FILE}. Please run fetch_page first."
        }

    try:
        with open(HTML_FILE, "r", encoding="utf-8") as f:
            page_html = f.read()

        tree = lxml_html.fromstring(page_html)

        # --- XPath Selectors for Amazon Product Details ---
        title = _extract_xpath(tree, '//span[@id="productTitle"]')
        # Handle different price structures (main price, sale price)
        price_whole = _extract_xpath(tree, '//span[contains(@class, "a-price-whole")]')
        price_fraction = _extract_xpath(
            tree, '//span[contains(@class, "a-price-fraction")]'
        )
        price_str = (
            f"{price_whole}.{price_fraction}"
            if price_whole != "N/A"
            else _extract_xpath(tree, '//span[contains(@class,"a-offscreen")]')
        )  # Fallback to offscreen if needed

        price = _extract_price(price_str)

        # Original price (strike-through)
        original_price_str = _extract_xpath(
            tree, '//span[@class="a-price a-text-price"]//span[@class="a-offscreen"]'
        )
        original_price = _extract_price(original_price_str)

        # Rating
        rating_text = _extract_xpath(tree, '//span[@id="acrPopover"]/@title')
        rating = None
        if rating_text != "N/A":
            try:
                rating = float(rating_text.split()[0])
            except (ValueError, IndexError):
                rating = None

        # Review Count
        reviews_text = _extract_xpath(tree, '//span[@id="acrCustomerReviewText"]')
        review_count = None
        if reviews_text != "N/A":
            try:
                review_count = int(reviews_text.split()[0].replace(",", ""))
            except (ValueError, IndexError):
                review_count = None

        # Availability
        availability = _extract_xpath(
            tree,
            '//div[@id="availability"]//span/text()',
        )

        # Features (bullet points)
        feature_elements = tree.xpath(
            '//div[@id="feature-bullets"]//li//span[@class="a-list-item"]'
        )
        features = [
            elem.text_content().strip()
            for elem in feature_elements
            if elem.text_content().strip()
        ]

        # Calculate Discount
        discount = None
        if price and original_price and original_price > price:
            discount = round(((original_price - price) / original_price) * 100)

        extracted_data = {
            "title": title,
            "price": price,
            "original_price": original_price,
            "discount_percent": discount,
            "rating_stars": rating,
            "review_count": review_count,
            "features": features,
            "availability": availability.strip(),
        }
        print(f"Successfully extracted data: {extracted_data}")
        return extracted_data

    except Exception as e:
        error_message = f"Error parsing HTML: {str(e)}"
        print(error_message)  # Added for logging
        return {"error": error_message}
```

ã“ã®é–¢æ•°ã¯ã€LXMLã®`fromstring`ã‚’ä½¿ç”¨ã—ã¦HTMLã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€å …ç‰¢ãªXPathã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§ç›®çš„ã®è¦ç´ ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚

### Step 5: Running the Server

æœ€å¾Œã«ã€`amazon_scraper_mcp.py`ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æœ«å°¾ã«æ¬¡ã®è¡Œã‚’è¿½åŠ ã—ã¦ã€`stdio`ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ãƒˆæ©Ÿæ§‹ã§serverã‚’èµ·å‹•ã—ã¾ã™ã€‚ã“ã‚Œã¯Claude Desktopã‚„Cursorã®ã‚ˆã†ãªclientã¨é€šä¿¡ã™ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«MCP serverã®æ¨™æº–ã§ã™ã€‚

```python
if __name__ == "__main__":
    print("Starting MCP Server with stdio transport...")
    # Run the server, listening via standard input/output
    mcp.run(transport="stdio")
```

### Complete Source Code

```python
import os
import asyncio
from lxml import html as lxml_html
from mcp.server.fastmcp import FastMCP
from playwright.async_api import async_playwright

# Define a temporary file path for the HTML content
HTML_FILE = os.path.join(os.getenv("TMPDIR", "/tmp"), "amazon_product_page.html")

# Initialize the MCP server with a descriptive name
mcp = FastMCP("Amazon Product Scraper")

print("MCP Server Initialized: Amazon Product Scraper")

@mcp.tool()
async def fetch_page(url: str) -> str:
    """
    Fetches the HTML content of the given Amazon product URL using Playwright
    and saves it to a temporary file. Returns a status message.
    """
    print(f"Executing fetch_page for URL: {url}")
    try:
        async with async_playwright() as p:
            # Launch headless Chromium browser
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            # Navigate to the URL with a generous timeout
            await page.goto(url, timeout=90000, wait_until="domcontentloaded")
            # Wait for a key element (e.g., body) to ensure basic loading
            await page.wait_for_selector("body", timeout=30000)
            # Add a small delay for any dynamic content rendering via JavaScript
            await asyncio.sleep(5)

            html_content = await page.content()
            with open(HTML_FILE, "w", encoding="utf-8") as f:
                f.write(html_content)

            await browser.close()
            print(f"Successfully fetched and saved HTML to {HTML_FILE}")
            return f"HTML content for {url} downloaded and saved successfully to {HTML_FILE}."
    except Exception as e:
        error_message = f"Error fetching page {url}: {str(e)}"
        print(error_message)
        return error_message

def _extract_xpath(tree, xpath, default="N/A"):
    """Helper function to extract text using XPath, returning default if not found."""
    try:
        # Use text_content() to get text from node and children, strip whitespace
        result = tree.xpath(xpath)
        if result:
            return result[0].text_content().strip()
        return default
    except Exception:
        return default

def _extract_price(price_str):
    """Helper function to parse price string into a float."""
    if price_str == "N/A":
        return None
    try:
        # Remove currency symbols and commas, handle potential whitespace
        cleaned_price = "".join(filter(str.isdigit or str.__eq__("."), price_str))
        return float(cleaned_price)
    except (ValueError, TypeError):
        return None

@mcp.tool()
def extract_info() -> dict:
    """
    Parses the saved HTML file (downloaded by fetch_page) to extract
    Amazon product details like title, price, rating, features, etc.
    Returns a dictionary of the extracted data.
    """
    print(f"Executing extract_info from file: {HTML_FILE}")
    if not os.path.exists(HTML_FILE):
        return {
            "error": f"HTML file not found at {HTML_FILE}. Please run fetch_page first."
        }

    try:
        with open(HTML_FILE, "r", encoding="utf-8") as f:
            page_html = f.read()

        tree = lxml_html.fromstring(page_html)

        # --- XPath Selectors for Amazon Product Details ---
        title = _extract_xpath(tree, '//span[@id="productTitle"]')
        # Handle different price structures (main price, sale price)
        price_whole = _extract_xpath(tree, '//span[contains(@class, "a-price-whole")]')
        price_fraction = _extract_xpath(
            tree, '//span[contains(@class, "a-price-fraction")]'
        )
        price_str = (
            f"{price_whole}.{price_fraction}"
            if price_whole != "N/A"
            else _extract_xpath(tree, '//span[contains(@class,"a-offscreen")]')
        )  # Fallback to offscreen if needed

        price = _extract_price(price_str)

        # Original price (strike-through)
        original_price_str = _extract_xpath(
            tree, '//span[@class="a-price a-text-price"]//span[@class="a-offscreen"]'
        )
        original_price = _extract_price(original_price_str)

        # Rating
        rating_text = _extract_xpath(tree, '//span[@id="acrPopover"]/@title')
        rating = None
        if rating_text != "N/A":
            try:
                rating = float(rating_text.split()[0])
            except (ValueError, IndexError):
                rating = None

        # Review Count
        reviews_text = _extract_xpath(tree, '//span[@id="acrCustomerReviewText"]')
        review_count = None
        if reviews_text != "N/A":
            try:
                review_count = int(reviews_text.split()[0].replace(",", ""))
            except (ValueError, IndexError):
                review_count = None

        # Availability
        availability = _extract_xpath(
            tree,
            '//div[@id="availability"]//span/text()',
        )

        # Features (bullet points)
        feature_elements = tree.xpath(
            '//div[@id="feature-bullets"]//li//span[@class="a-list-item"]'
        )
        features = [
            elem.text_content().strip()
            for elem in feature_elements
            if elem.text_content().strip()
        ]

        # Calculate Discount
        discount = None
        if price and original_price and original_price > price:
            discount = round(((original_price - price) / original_price) * 100)

        extracted_data = {
            "title": title,
            "price": price,
            "original_price": original_price,
            "discount_percent": discount,
            "rating_stars": rating,
            "review_count": review_count,
            "features": features,
            "availability": availability.strip(),
        }
        print(f"Successfully extracted data: {extracted_data}")
        return extracted_data

    except Exception as e:
        error_message = f"Error parsing HTML: {str(e)}"
        print(error_message)  # Added for logging
        return {"error": error_message}

if __name__ == "__main__":
    print("Starting MCP Server with stdio transport...")
    # Run the server, listening via standard input/output
    mcp.run(transport="stdio")
```

## Connecting Your MCP Server

serverã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æº–å‚™ãŒã§ããŸã®ã§ã€Claude Desktopã‚„Cursorã®ã‚ˆã†ãªMCP clientã«æ¥ç¶šã—ã¾ã—ã‚‡ã†ã€‚

### Setting Up with Claude Desktop

**Step 1:** Claude Desktopã‚’é–‹ãã¾ã™ã€‚

**Step 2:** `Settings` -> `Developer` -> `Edit Config`ã«ç§»å‹•ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã§`claude_desktop_config.json`ãƒ•ã‚¡ã‚¤ãƒ«ãŒé–‹ãã¾ã™ã€‚

![claude-desktop-settings-menu-navigation](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/claude-desktop-settings-menu-navigation.png)

**Step 3:** `mcpServers`ã‚­ãƒ¼é…ä¸‹ã«serverã®ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ ã—ã¾ã™ã€‚`args`ã®ãƒ‘ã‚¹ã¯ã€`amazon_scraper_mcp.py`ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®çµ¶å¯¾ãƒ‘ã‚¹ã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚

```json
{
  "mcpServers": {
    "amazon_product_scraper": {
      "command": "python",  // Or python3 if needed
      "args": ["/full/path/to/your/amazon_scraper_mcp.py"], // <-- IMPORTANT: Use the correct absolute path
    }
  }
}
```

**Step 4:** `claude_desktop_config.json`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã€å¤‰æ›´ã‚’åæ˜ ã™ã‚‹ãŸã‚ã«Claude Desktopã‚’å®Œå…¨ã«çµ‚äº†ã—ã¦å†èµ·å‹•ã—ã¾ã™ã€‚

**Step 5:** Claude Desktopã®ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã‚¨ãƒªã‚¢ã«ã€å°ã•ãªtoolsã‚¢ã‚¤ã‚³ãƒ³ï¼ˆãƒãƒ³ãƒãƒ¼ğŸ”¨ã®ã‚ˆã†ãªã‚‚ã®ï¼‰ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¯ãšã§ã™ã€‚

![claude-desktop-mcp-tools-icon-interface](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/claude-desktop-mcp-tools-icon-interface.png)

**Step 6:** ãã‚Œã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€`fetch_page`ã¨`extract_info`ã®toolsã‚’å‚™ãˆãŸã€ŒAmazon Product Scraperã€ãŒä¸€è¦§ã«è¡¨ç¤ºã•ã‚Œã‚‹ã¯ãšã§ã™ã€‚

![claude-available-mcp-tools-dialog-amazon-scraper](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/claude-available-mcp-tools-dialog-amazon-scraper.png)

**Step 7:** ä¾‹ãˆã°æ¬¡ã®ã‚ˆã†ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡ã—ã¾ã™: _"Get the current price, original price, and rating for this Amazon product: [https://www.amazon.com/dp/B09C13PZX7](https://www.amazon.com/dp/B09C13PZX7)"._

**Step 8:** Claudeã¯å¤–éƒ¨toolsãŒå¿…è¦ã ã¨æ¤œçŸ¥ã—ã€ã¾ãš`fetch_page`ã€æ¬¡ã«`extract_info`ã‚’å®Ÿè¡Œã™ã‚‹è¨±å¯ã‚’æ±‚ã‚ã¾ã™ã€‚å„toolã«ã¤ã„ã¦ã€ŒAllow for this chatã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚

![mcp-permission-dialog-fetch-page-amazon-tool](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/mcp-permission-dialog-fetch-page-amazon-tool.png)

**Step 9:** æ¨©é™ã‚’ä»˜ä¸ã™ã‚‹ã¨ã€MCP serverãŒtoolsã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ãã®å¾ŒClaudeã¯æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã€ãƒãƒ£ãƒƒãƒˆå†…ã«æç¤ºã—ã¾ã™ã€‚

![claude-amazon-product-data-extraction-results](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/claude-amazon-product-data-extraction-results-2.png)

### Setting Up with Cursor

Cursorï¼ˆAIãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆIDEï¼‰ã®æ‰‹é †ã‚‚åŒæ§˜ã§ã™ã€‚

**Step 1:** Cursorã‚’é–‹ãã¾ã™ã€‚

**Step 2:** `Settings` âš™ï¸ã¸é€²ã¿ã€`MCP`ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ç§»å‹•ã—ã¾ã™ã€‚

![cursor-ide-add-new-global-mcp-server-settings](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/cursor-ide-add-new-global-mcp-server-settings.png)

**Step 3:** ã€Œ+Add a new global MCP Serverã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Š`mcp.json`è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒé–‹ãã¾ã™ã€‚serverã®ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ ã—ã€ã“ã“ã§ã‚‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¸ã®**çµ¶å¯¾ãƒ‘ã‚¹**ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

![cursor-mcp-json-configuration-file-amazon-scraper](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/cursor-mcp-json-configuration-file-amazon-scraper.png)

**Step 4:** `mcp.json`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã¨ã€ã€Œamazon\_product\_scraperã€ãŒä¸€è¦§ã«è¡¨ç¤ºã•ã‚Œã€èµ·å‹•ãƒ»æ¥ç¶šã•ã‚Œã¦ã„ã‚Œã°ç·‘ã®ãƒ‰ãƒƒãƒˆãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¯ãšã§ã™ã€‚

![cursor-ide-configured-amazon-scraper-mcp-settings](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/cursor-ide-configured-amazon-scraper-mcp-settings.png)

**Step 5:** Cursorã®ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ï¼ˆ`Cmd+l`ã¾ãŸã¯`Ctrl+l`ï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

**Step 6:** ä¾‹ãˆã°æ¬¡ã®ã‚ˆã†ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡ã—ã¾ã™: "_Extract all available product data from this Amazon URL: [https://www.amazon.com/dp/B09C13PZX7](https://www.amazon.com/dp/B09C13PZX7). Format the output as a structured JSON object"_.

**Step 7:** Claude Desktopã¨åŒæ§˜ã«ã€Cursorã¯`fetch_page`ã¨`extract_info`ã‚’å®Ÿè¡Œã™ã‚‹æ¨©é™ã‚’æ±‚ã‚ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆã€ŒRun Toolã€ï¼‰ã‚’æ‰¿èªã—ã¾ã™ã€‚

**Step 8:** Cursorã¯å¯¾è©±ãƒ•ãƒ­ãƒ¼ã‚’è¡¨ç¤ºã—ã€MCP toolsã®å‘¼ã³å‡ºã—ã€æœ€å¾Œã«`extract_info` toolãŒè¿”ã—ãŸæ§‹é€ åŒ–JSONãƒ‡ãƒ¼ã‚¿ã‚’æç¤ºã—ã¾ã™ã€‚

![cursor-ide-amazon-product-data-extraction-json-results](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/cursor-ide-amazon-product-data-extraction-json-results.png)
ä»¥ä¸‹ã¯Cursorã‹ã‚‰ã®JSONå‡ºåŠ›ä¾‹ã§ã™ã€‚

```json
{
  "title": "Razer Basilisk V3 Customizable Ergonomic Gaming Mouse: Fastest Gaming Mouse Switch - Chroma RGB Lighting - 26K DPI Optical Sensor - 11 Programmable Buttons - HyperScroll Tilt Wheel - Classic Black",
  "price": 39.99,
  "original_price": 69.99,
  "discount_percent": 43,
  "rating_stars": 4.6,
  "review_count": 7782,
  "features": [
    "ICONIC ERGONOMIC DESIGN WITH THUMB REST â€” PC gaming mouse favored by millions worldwide with a form factor that perfectly supports the hand while its buttons are optimally positioned for quick and easy access",
    "11 PROGRAMMABLE BUTTONS â€” Assign macros and secondary functions across 11 programmable buttons to execute essential actions like push-to-talk, ping, and more",
    "HYPERSCROLL TILT WHEEL â€” Speed through content with a scroll wheel that free-spins until its stopped or switch to tactile mode for more precision and satisfying feedback that's ideal for cycling through weapons or skills",
    "11 RAZER CHROMA RGB LIGHTING ZONES â€” Customize each zone from over 16.8 million colors and countless lighting effects, all while it reacts dynamically with over 150 Chroma integrated games",
    "OPTICAL MOUSE SWITCHES GEN 2 â€” With zero unintended misclicks these switches provide crisp, responsive execution at a blistering 0.2ms actuation speed for up to 70 million clicks",
    "FOCUS+ 26K DPI OPTICAL SENSOR â€” Best-in-class mouse sensor with intelligent functions flawlessly tracks movement with zero smoothing, allowing for crisp response and pixel-precise accuracy",
    // ... (other features)
  ],
  "availability": "In Stock"
}
```

ã“ã‚Œã¯MCPã®æŸ”è»Ÿæ€§ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚åŒã˜serverãŒã€ç•°ãªã‚‹clientã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚‚ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«å‹•ä½œã—ã¾ã™ã€‚

## Using Bright Data's MCP for Professional Web Data Extraction

Bright Dataã®ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ã®[Model Context Protocolï¼ˆMCPï¼‰](https://github.com/bright-jp/brightdata-mcp)ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€è‡ªå·±ç®¡ç†ã®MCP serverã«ä¼´ã†è¤‡é›‘ã•ï¼ˆãƒ—ãƒ­ã‚­ã‚·ç®¡ç†ã€[ã‚¢ãƒ³ãƒãƒœãƒƒãƒˆå›é¿ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³](https://brightdata.jp/blog/web-data/anti-scraping-techniques)ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®èª²é¡Œãªã©ï¼‰ã‚’è§£æ¶ˆã—ã€[AI agents](https://brightdata.jp/use-cases/apps-agents)ãŠã‚ˆã³LLMã¨ã®ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªçµ±åˆã‚’æä¾›ã—ã¾ã™ã€‚

Bright Dataã®MCPã«æ¥ç¶šã™ã‚‹ã¨ã€SERPçµæœã‚„åˆ°é”ãŒé›£ã—ã„ã‚µã‚¤ãƒˆã‚’å«ã‚€ãƒ‘ãƒ–ãƒªãƒƒã‚¯Webãƒ‡ãƒ¼ã‚¿ã¸å³æ™‚ã‚¢ã‚¯ã‚»ã‚¹ã§ãã€AIãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å‘ã‘ã«æœ€é©åŒ–ã•ã‚Œã¾ã™ã€‚

MCPã¯ã€[Web Unlocker](https://brightdata.jp/products/web-unlocker)ã€[SERP API](https://brightdata.jp/products/serp-api)ã€[Web Scraper API](https://brightdata.jp/products/web-scraper)ã€[Scraping Browser](https://brightdata.jp/products/scraping-browser)ãªã©ã®ãƒ„ãƒ¼ãƒ«ã«ã‚ˆã£ã¦å¼·åŠ›ãªWebæŠ½å‡ºãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’è§£æ”¾ã—ã€æ¬¡ã‚’æä¾›ã—ã¾ã™ã€‚

- **[AI-Ready Data](https://brightdata.jp/use-cases/data-for-ai):** äº‹å‰ã«æ§‹é€ åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§ã€å‰å‡¦ç†ã¯ä¸è¦ã§ã™ã€‚
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã¨ä¿¡é ¼æ€§:** é«˜ãƒœãƒªãƒ¥ãƒ¼ãƒ ã§ã‚‚é€Ÿåº¦ä½ä¸‹ãªãå¯¾å¿œã—ã¾ã™ã€‚
- **ãƒ–ãƒ­ãƒƒã‚¯ãŠã‚ˆã³CAPTCHAå›é¿:** é«˜åº¦ãªã‚¢ãƒ³ãƒãƒœãƒƒãƒˆæ©Ÿèƒ½ã§ã™ã€‚
- **ã‚°ãƒ­ãƒ¼ãƒãƒ«IPã‚«ãƒãƒ¬ãƒƒã‚¸:** [Bright Data proxies](https://brightdata.jp/proxy-types)ã§195ã‹å›½ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã™ã€‚
- **ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªçµ±åˆ:** ã©ã®MCP clientã§ã‚‚è¿…é€Ÿã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ãã¾ã™ã€‚

### Prerequisites for Bright Data MCP

Bright Dataã®MCPçµ±åˆã‚’é–‹å§‹ã™ã‚‹å‰ã«ã€ä»¥ä¸‹ãŒæƒã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

1. **Bright Dataã‚¢ã‚«ã‚¦ãƒ³ãƒˆ:** [brightdata.com](https://brightdata.jp/)ã§ç™»éŒ²ã—ã¦ãã ã•ã„ã€‚åˆå›ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯ãƒ†ã‚¹ãƒˆç”¨ã®ç„¡æ–™ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆãŒä»˜ä¸ã•ã‚Œã¾ã™ã€‚
2. **API Token:** Bright Dataã‚¢ã‚«ã‚¦ãƒ³ãƒˆè¨­å®šã‹ã‚‰API tokenã‚’å®‰å…¨ã«å–å¾—ã—ã¦ãã ã•ã„ï¼ˆ[User Settings Page](https://brightdata.jp/cp/setting/users)ï¼‰ã€‚
3. **Web Unlocker Zone:** Bright Dataã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ‘ãƒãƒ«ã§[Web Unlocker proxy](https://docs.brightdata.com/scraping-automation/web-unlocker/quickstart) zoneã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚`mcp_unlocker`ã®ã‚ˆã†ãªè¦šãˆã‚„ã™ã„è­˜åˆ¥å­ã‚’é¸ã³ã¾ã™ï¼ˆå¿…è¦ã«å¿œã˜ã¦ã€å¾Œã§ç’°å¢ƒå¤‰æ•°ã§å¤‰æ›´ã§ãã¾ã™ï¼‰ã€‚
4. **(Optional) Scraping Browser Zone:** é«˜åº¦ãªãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–æ©Ÿèƒ½ï¼ˆä¾‹: è¤‡é›‘ãªJavaScriptæ“ä½œã‚„ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼‰ãŒå¿…è¦ãªå ´åˆã¯ã€[Scraping Browser zone](https://docs.brightdata.com/scraping-automation/scraping-browser/quickstart)ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚ã“ã®zoneã«æä¾›ã•ã‚Œã‚‹èªè¨¼æƒ…å ±ï¼ˆUsernameã¨Passwordã€‚**Overview**ã‚¿ãƒ–å†…ï¼‰ã‚’æ§ãˆã¦ãŠãã¾ã™ã€‚ä¸€èˆ¬çš„ã«`brd-customer-ACCOUNT_ID-zone-ZONE_NAME:PASSWORD`ã®å½¢å¼ã§ã™ã€‚

### Quickstart: Configuring Bright Data MCP for Claude Desktop

**Step 1:** Bright Dataã®MCP serverã¯é€šå¸¸`npx`ã§å®Ÿè¡Œã—ã¾ã™ï¼ˆNode.jsã«åŒæ¢±ï¼‰ã€‚å¿…è¦ã«å¿œã˜ã¦[å…¬å¼ã‚µã‚¤ãƒˆ](https://nodejs.org/en/download)ã‹ã‚‰Node.jsã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚

**Step 2:** Claude Desktop -> `Settings` -> `Developer` -> `Edit Config`ï¼ˆ`claude_desktop_config.json`ï¼‰ã‚’é–‹ãã¾ã™ã€‚

**Step 3:** `mcpServers`é…ä¸‹ã«Bright Data serverè¨­å®šã‚’æŒ¿å…¥ã—ã¾ã™ã€‚ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’å®Ÿéš›ã®èªè¨¼æƒ…å ±ã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚

```json
{
  "mcpServers": {
    "Bright Data": { // Choose a name for the server
      "command": "npx",
      "args": ["@brightdata/mcp"],
      "env": {
        "API_TOKEN": "YOUR_BRIGHTDATA_API_TOKEN", // Paste your API token here
        "WEB_UNLOCKER_ZONE": "mcp_unlocker",     // Your Web Unlocker zone name
        // Optional: Add if using Scraping Browser tools
        "BROWSER_AUTH": "brd-customer-ACCOUNTID-zone-YOURZONE:PASSWORD"
      }
    }
  }
}
```

**Step 4:** è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã€Claude Desktopã‚’å†èµ·å‹•ã—ã¾ã™ã€‚

**Step 5:** Claude Desktopã®ãƒãƒ³ãƒãƒ¼ã‚¢ã‚¤ã‚³ãƒ³ï¼ˆğŸ”¨ï¼‰ã«ã‚«ãƒ¼ã‚½ãƒ«ã‚’åˆã‚ã›ã¾ã™ã€‚è¤‡æ•°ã®MCP toolsãŒåˆ©ç”¨å¯èƒ½ã«ãªã£ã¦ã„ã‚‹ã¯ãšã§ã™ã€‚

![claude-desktop-interface-with-mcp-tools-available](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/claude-desktop-interface-with-mcp-tools-available.png)

ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’åˆ¶é™ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã§çŸ¥ã‚‰ã‚Œã‚‹Webã‚µã‚¤ãƒˆã§ã‚ã‚‹Zillowã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚Claudeã«æ¬¡ã®ã‚ˆã†ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¾ã™: "_Extract key property data in JSON format from this Zillow URL: [https://www.zillow.com/apartments/arverne-ny/the-tides-at-arverne-by-the-sea/ChWHPZ/](https://www.zillow.com/apartments/arverne-ny/the-tides-at-arverne-by-the-sea/ChWHPZ/)_"

![bright-data-mcp-zillow-property-extraction-process](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/bright-data-mcp-zillow-property-extraction-process.png)

å¿…è¦ãªBright Dataã®MCP toolsã‚’ClaudeãŒåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’è¨±å¯ã—ã¦ãã ã•ã„ã€‚Bright Dataã®MCP serverãŒã€åŸºç›¤ã¨ãªã‚‹è¤‡é›‘ã•ï¼ˆãƒ—ãƒ­ã‚­ã‚·ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€å¿…è¦ã«å¿œã˜ãŸScraping Browserã«ã‚ˆã‚‹JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ï¼‰ã‚’ç®¡ç†ã—ã¾ã™ã€‚

Bright Dataã®serverãŒæŠ½å‡ºã‚’å®Ÿæ–½ã—ã€æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã€ClaudeãŒãã‚Œã‚’æç¤ºã—ã¾ã™ã€‚

![zillow-property-data-json-structure-bright-data-mcp](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/zillow-property-data-json-structure-bright-data-mcp.png)

ä»¥ä¸‹ã¯æƒ³å®šã•ã‚Œã‚‹å‡ºåŠ›ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚

```json
{
  "propertyInfo": {
    "name": "The Tides At Arverne By The Sea",
    "address": "190 Beach 69th St, Arverne, NY 11692",
    "propertyType": "Apartment building",
    // ... more info
  },
  "rentPrices": {
    "studio": { "startingPrice": "$2,750", /* ... */ },
    "oneBed": { "startingPrice": "$2,900", /* ... */ },
    "twoBed": { "startingPrice": "$3,350", /* ... */ }
  },
  // ... amenities, policies, etc.
}
```

**åˆ¥ã®ä¾‹: Hacker Newsã®è¦‹å‡ºã—**

ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ãªã‚¯ã‚¨ãƒªã¨ã—ã¦ã€æ¬¡ã®ã‚ˆã†ã«ä¾é ¼ã—ã¾ã™: "_Give me the titles of the latest 5 news articles from Hacker News_".

![hacker-news-latest-articles-mcp-extraction-results](https://github.com/bright-jp/web-scraping-with-mcp/blob/main/images/hacker-news-latest-articles-mcp-extraction-results.png)

ã“ã‚Œã¯ã€Bright Dataã®MCP serverãŒã€å‹•çš„ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚„å¼·å›ºã«ä¿è­·ã•ã‚ŒãŸWebã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§ã‚ã£ã¦ã‚‚ã€AIãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å†…ã§ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ã‚’ã©ã®ã‚ˆã†ã«ç°¡ç´ åŒ–ã™ã‚‹ã‹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

## Further Reading

ã‚ˆã‚Šæ·±ã„çŸ¥è­˜ã®ãŸã‚ã«ã€AIãŠã‚ˆã³å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã«é–¢ã™ã‚‹éå»ã®ã‚¬ã‚¤ãƒ‰ã‚’å³é¸ã—ã¦ã”ç´¹ä»‹ã—ã¾ã™ã€‚

- [Top Sources for Finding LLM Training Data](https://brightdata.jp/blog/web-data/llm-training-data)
- [Web Scraping with LLaMA 3: Turn Any Website into Structured JSON](https://brightdata.jp/blog/web-data/web-scraping-with-llama-3)
- [Web Scraping With LangChain and Bright Data](https://brightdata.jp/blog/web-data/web-scraping-with-langchain-and-bright-data)
- [How To Create a RAG Chatbot With GPT-4o Using SERP Data](https://brightdata.jp/blog/web-data/build-a-rag-chatbot)

## Conclusion

Anthropicã®Model Context Protocolã¯ã€AIã‚·ã‚¹ãƒ†ãƒ ãŒå¤–éƒ¨ä¸–ç•Œã¨ç›¸äº’ä½œç”¨ã™ã‚‹æ–¹æ³•ã«ãŠã‘ã‚‹æ ¹æœ¬çš„ãªè»¢æ›ç‚¹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ç‰¹å®šã‚¿ã‚¹ã‚¯å‘ã‘ã«ã‚«ã‚¹ã‚¿ãƒ MCP serverã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚Bright Dataã®MCPçµ±åˆã¯ã“ã‚Œã‚’ã•ã‚‰ã«å¼·åŒ–ã—ã€ã‚¢ãƒ³ãƒãƒœãƒƒãƒˆä¿è­·ã‚’å›é¿ã—ã€[AI-readyãªæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿](https://brightdata.jp/use-cases/data-for-ai)ã‚’ä¾›çµ¦ã™ã‚‹ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ã®Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

ä»Šã™ã[AI solutions](https://brightdata.jp/ai)ã«ç™»éŒ²ã—ã¦ã€ç„¡æ–™ã§ãŠè©¦ã—ãã ã•ã„ï¼